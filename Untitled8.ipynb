{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDHBwnH8H5lym8sxV4SUId",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/furio1999/Cycling-racing/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridoaSxhVNxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a79659-59f7-4702-9726-058b5871e2ec"
      },
      "source": [
        "import requests\n",
        "# !pip install --upgrade beautifulsoup4\n",
        "# !pip install --upgrade html5lib\n",
        "!pip install chart_studio\n",
        "!pip install tabula-py\n",
        "!pip install tabulate\n",
        "!pip install PyPDF2\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup\n",
        "from string import ascii_letters\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import lxml.html as lh #tables\n",
        "import chart_studio.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import io\n",
        "from PyPDF2 import PdfFileReader\n",
        "\n",
        "\n",
        "from tabula import read_pdf\n",
        "from tabulate import tabulate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting chart_studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from chart_studio) (4.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chart_studio) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.3.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2021.5.30)\n",
            "Installing collected packages: chart-studio\n",
            "Successfully installed chart-studio-1.1.0\n",
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.2.0-py3-none-any.whl (11.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.7 MB 231 kB/s \n",
            "\u001b[?25hCollecting distro\n",
            "  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->tabula-py) (1.15.0)\n",
            "Installing collected packages: distro, tabula-py\n",
            "Successfully installed distro-1.6.0 tabula-py-2.2.0\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9)\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 4.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61100 sha256=f736eff9b024cab9902a769f44911968c471c7ac9697e6aa5b6036b1883b9b93\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRoezBF6oau6"
      },
      "source": [
        "# Scraping ProcyclingStats (working version)\n",
        "Credits to josselingirault for the initial webscraper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0MLH1mkohJd",
        "outputId": "f4f2fb50-8d3d-47e6-9e54-fc0aaa1bd89a"
      },
      "source": [
        "# !git clone https://github.com/furio1999/procyclingstats.com-webscraper.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'procyclingstats.com-webscraper'...\n",
            "remote: Enumerating objects: 147, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 147 (delta 0), reused 0 (delta 0), pack-reused 146\u001b[K\n",
            "Receiving objects: 100% (147/147), 968.73 KiB | 5.04 MiB/s, done.\n",
            "Resolving deltas: 100% (72/72), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_XeDAVHpJoS"
      },
      "source": [
        "########    FORBIDDEN!!!!   ######\n",
        "######################################################\n",
        "\n",
        "# # Library for opening url and creating\n",
        "# # requests\n",
        "# import urllib.request\n",
        "\n",
        "# # pretty-print python data structures\n",
        "# from pprint import pprint\n",
        "\n",
        "# # for parsing all the tables present\n",
        "# # on the website\n",
        "# !pip install html_table_parser\n",
        "# from html_table_parser import HTMLTableParser #problems\n",
        "\n",
        "# # for converting the parsed data in a\n",
        "# # pandas dataframe\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# # Opens a website and read its\n",
        "# # binary contents (HTTP Response Body)\n",
        "# def url_get_contents(url):\n",
        "\n",
        "# \t# Opens a website and read its\n",
        "# \t# binary contents (HTTP Response Body)\n",
        "\n",
        "# \t#making request to the website\n",
        "# \treq = urllib.request.Request(url=url)\n",
        "# \tf = urllib.request.urlopen(req)\n",
        "\n",
        "# \t#reading contents of the website\n",
        "# \treturn f.read()\n",
        "\n",
        "# # defining the html contents of a URL.\n",
        "# xhtml = url_get_contents('https://www.procyclingstats.com/rider/primoz-roglic/2019').decode('utf-8')\n",
        "\n",
        "# # Defining the HTMLTableParser object\n",
        "# p = HTMLTableParser()\n",
        "\n",
        "# feeding the html contents in the\n",
        "# HTMLTableParser object\n",
        "p.feed(xhtml)\n",
        "\n",
        "# Now finally obtaining the data of\n",
        "# the table required\n",
        "pprint(p.tables[1])\n",
        "\n",
        "# converting the parsed data to\n",
        "# datframe\n",
        "print(\"\\n\\nPANDAS DATAFRAME\\n\")\n",
        "print(pd.DataFrame(p.tables[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm2pfYhQPz5B"
      },
      "source": [
        "#n-th column\n",
        "\n",
        "def remove_items(test_list, item):\n",
        "      \n",
        "    # using list comprehension to perform the task\n",
        "    res = [i for i in test_list if i != item]\n",
        "  \n",
        "    return res\n",
        "\n",
        "def get_tabletitle(url):\n",
        " data = requests.get(url)\n",
        " soup = BeautifulSoup(data.text, 'html.parser')\n",
        " table=soup.find('table', {\"class\":\"rdrResults\"})\n",
        " ths=soup.find_all(\"th\")\n",
        " cols=[ths[i].text for i in range(0, len(ths))]\n",
        " cols=remove_items(cols, \"\")\n",
        " cols=remove_items(cols, '')\n",
        " \n",
        " print(cols)\n",
        "\n",
        " return cols\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  df = pd.DataFrame(row, columns=titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bt2FfsbvYiQ"
      },
      "source": [
        "def get_raw(scraped):\n",
        " raws=[scraped[i].text for i in range(0, len(scraped))]\n",
        " #  raws=remove_items(raws, \"\")\n",
        " #  raws=remove_items(raws, '')\n",
        " return raws \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liYHSHY7eeA6"
      },
      "source": [
        "#delete 12>13 and ''\n",
        "#col1 e col2 è riferito alla tabella del sito, non a tdtot. forse sono da invertire\n",
        "#oppure posso riferire col come le colonne della tabella di output. verifica se ora è così\n",
        "#splitta chrono des... (1.1) in due voci nel preprocessing\n",
        "#enable option from..to or single shot both raw and col\n",
        "#if targets=None and no info su col e rows deve restituire la tabella completa in formato pd.datarame\n",
        "def get_table_params(url, targets_raw=None, targets=None, row1=0, row2=6, col1=1, col2=6):\n",
        " data = requests.get(url)\n",
        " soup = BeautifulSoup(data.text, 'html.parser')\n",
        " table=soup.find('table', {\"class\":\"rdrResults\"}) #è semplice, pulito ma non è scalabile. Come posso trovare ciò che mi serve da input oppure autonomamente? Senza sapere i nomi \"nascosti\" in html\n",
        " rows=table.find_all(\"tr\") #vorrei un if(option=True) che conta i tr di una gara a tappe (classifiche finali) come uno.\n",
        " tdtot=[tr.find_all(\"td\") for tr in rows]\n",
        " length1=len(rows)\n",
        " length=len(tdtot)\n",
        " cols=get_tabletitle(url)\n",
        " #  raws=get_raw(rows)\n",
        "\n",
        "\n",
        " if targets is not None:\n",
        "\n",
        "   col1=cols.index(targets[0])+1\n",
        "   col2=cols.index(targets[1])+1 #tdtot strats from 1, I must make it start from 0\n",
        "\n",
        "## colonne di tdtot, non della tabella vera (verifica)\n",
        "\n",
        " if col2<col1:\n",
        "   temp=col1\n",
        "   col1=col2\n",
        "   col2=temp\n",
        "\n",
        " if row2<row1:\n",
        "   temp=row1\n",
        "   row1=row2\n",
        "   row2=temp\n",
        "\n",
        " print(col1)\n",
        " print(col2)\n",
        "\n",
        " ##invert columns rows ##\n",
        "\n",
        " ###            ###\n",
        "\n",
        "\n",
        " row=row2-row1\n",
        " col=col2-col1\n",
        "\n",
        " if col < length and col>0:\n",
        "   col2=col2+1 #col2=num colonna finale che voglio\n",
        " else:\n",
        "   print(\"excessive length\")\n",
        "   col2=length+1\n",
        "   col1=1 #si parte da 1\n",
        " if row < length1:\n",
        "   row2=row2+1\n",
        "   pass\n",
        " else:\n",
        "   print(\"too much rows \\n\")\n",
        "   row1=0\n",
        "   row2=length1+1\n",
        "\n",
        " dati=[]\n",
        " ## inverti i for ed ottieni array di columns\n",
        " for i in range(col1, col2):  #non voglio trovare la riga e la colonna in base ai numeri come input, ma in base ai nomi delle colonne (es data) ed alla chiave della riga (es 20.10)\n",
        "  date=[tdtot[i][row].text for row in range(row1, row2)] #create a new command to delete a table like ['', '', '']\n",
        "\n",
        "  # date=remove_items(date, '') not necessary\n",
        "  dati.append(date)\n",
        "\n",
        " raws=dati\n",
        " raws=[remove_items(raws[i], '') for i in range(0, len(raws))]\n",
        " raws=remove_items(raws, [])\n",
        " print(raws)\n",
        " if targets_raw is not None:\n",
        "   dati=[]\n",
        "   for row in raws:\n",
        "    index=raws.index(row)\n",
        "    try:\n",
        "     index1=row.index(targets_raw[0])\n",
        "     dati.append(raws[index][:])\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "     index2=row.index(targets_raw[0])\n",
        "     dati.append(raws[index][:])\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "\n",
        " ##aggiungi la riga title della table nel pd.dataframe\n",
        "\n",
        " print(date)\n",
        " print(len(date))\n",
        " print(dati)\n",
        "\n",
        " return raws\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkxmAoBd8L0l",
        "outputId": "79a339a0-0dec-447d-fb60-5f119da31010"
      },
      "source": [
        "url=\"https://www.procyclingstats.com/rider/primoz-roglic/2019/\"\n",
        "get_tabletitle(url)\n",
        "targets=[\"Result\" , \"Date\"]\n",
        "targets_raw=[\"Chrono des Nations (1.1)\", \"Tre Valli Varesine (1.HC)\"]\n",
        "data=get_table_params(url, targets_raw=targets_raw) #se non do assegno niente alla funzione ritorna get response 200"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Date', 'Result', 'Race', 'Distance', 'PointsPCS', 'PointsUCI', 'Points', 'position']\n",
            "['Date', 'Result', 'Race', 'Distance', 'PointsPCS', 'PointsUCI', 'Points', 'position']\n",
            "1\n",
            "6\n",
            "[['20.10', '3', 'Chrono des Nations (1.1)', '46.3', '40'], ['12.10', '7', 'Il Lombardia (1.UWT)', '243', '80'], ['08.10', '1', 'Tre Valli Varesine (1.HC)', '197.8', '125'], ['05.10', '1', \"Giro dell'Emilia (1.HC)\", '207.4', '125'], ['29.09', 'DNF', 'World Championships - Road Race (WC)', '260.7'], ['25.09', '12', 'World Championships - ITT (WC)', '54', '30']]\n",
            "['25.09', '12', '', '', 'World Championships - ITT (WC)', '54', '30']\n",
            "7\n",
            "[['20.10', '3', 'Chrono des Nations (1.1)', '46.3', '40'], ['20.10', '3', 'Chrono des Nations (1.1)', '46.3', '40']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}